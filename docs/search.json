[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "From Raw Data to Insight: A Python Data Analysis Workflow\n\n\n\npython\n\ndata analysis\n\ntutorial\n\n\n\n\n\n\n\n\n\nFeb 20, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "",
    "text": "I’m a Statistics student at BYU taking STAT 386: Data Science Process.\nThis site is my course portfolio, where I share: - Data analysis projects - Tutorial-style blog posts - My final project for the course\nUse the navigation bar to explore my work."
  },
  {
    "objectID": "index.html#hi-im-golden",
    "href": "index.html#hi-im-golden",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "",
    "text": "I’m a Statistics student at BYU taking STAT 386: Data Science Process.\nThis site is my course portfolio, where I share: - Data analysis projects - Tutorial-style blog posts - My final project for the course\nUse the navigation bar to explore my work."
  },
  {
    "objectID": "index.html#latest-tutorial",
    "href": "index.html#latest-tutorial",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "Latest Tutorial",
    "text": "Latest Tutorial\nFrom Raw Data to Insight: A Python Data Analysis Workflow\nA repeatable Python workflow for exploratory data analysis — loading data, cleaning, summarizing, and visualizing with pandas, matplotlib, and seaborn."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html",
    "href": "posts/python-data-workflow-tutorial.html",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "",
    "text": "Every data science project starts the same way: you have a messy dataset and a question you want to answer. The gap between those two things is the data analysis workflow—the sequence of steps that takes you from raw numbers in a spreadsheet to a clear, communicable insight.\nIn this tutorial, I’ll walk you through a repeatable Python workflow for exploratory data analysis (EDA). We’ll cover loading data, inspecting and cleaning it, computing summary statistics, and creating visualizations. By the end, you’ll have a practical template you can apply to almost any dataset you encounter, whether it comes from a CSV file, an API, or a database export.\nThe tools we’ll use are pandas for data manipulation and matplotlib plus seaborn for visualization. If you’ve written any Python before, you’ll find these libraries approachable, and the patterns we establish here transfer directly into more advanced machine learning pipelines later on."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#introduction",
    "href": "posts/python-data-workflow-tutorial.html#introduction",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "",
    "text": "Every data science project starts the same way: you have a messy dataset and a question you want to answer. The gap between those two things is the data analysis workflow—the sequence of steps that takes you from raw numbers in a spreadsheet to a clear, communicable insight.\nIn this tutorial, I’ll walk you through a repeatable Python workflow for exploratory data analysis (EDA). We’ll cover loading data, inspecting and cleaning it, computing summary statistics, and creating visualizations. By the end, you’ll have a practical template you can apply to almost any dataset you encounter, whether it comes from a CSV file, an API, or a database export.\nThe tools we’ll use are pandas for data manipulation and matplotlib plus seaborn for visualization. If you’ve written any Python before, you’ll find these libraries approachable, and the patterns we establish here transfer directly into more advanced machine learning pipelines later on."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#prerequisites",
    "href": "posts/python-data-workflow-tutorial.html#prerequisites",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore running any code, make sure the following are in place.\nYou should have Python 3.9 or newer installed. The quickest way to manage packages is with pip or a project-specific virtual environment. Install the required libraries with:\n# Install dependencies (run once in your terminal, not inside a script)\n# pip install pandas matplotlib seaborn\nYou’ll also need a dataset. For this tutorial, we’ll use the well-known Iris dataset, which ships with seaborn and requires no external download. The same steps apply to any tabular data you load from a local CSV."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-1-load-your-data",
    "href": "posts/python-data-workflow-tutorial.html#step-1-load-your-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 1: Load Your Data",
    "text": "Step 1: Load Your Data\nThe first step is always getting data into Python. For a CSV file you’ve downloaded locally, use pandas.read_csv. For data bundled with a library like seaborn, use its built-in loader.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Iris dataset (built into seaborn — no file needed)\ndf = sns.load_dataset(\"iris\")\n\n# Confirm it loaded correctly\nprint(df.shape)   # (150, 5): 150 rows, 5 columns\nprint(df.head())  # Preview first five rows\nLoading the data is simple, but don’t skip the .shape and .head() calls. These two lines tell you immediately whether the data arrived intact—correct number of rows, recognizable column names, and plausible values. Skipping them is how you end up debugging mysterious errors twenty steps later."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-2-inspect-and-clean-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-2-inspect-and-clean-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 2: Inspect and Clean the Data",
    "text": "Step 2: Inspect and Clean the Data\nRaw data almost always has issues: missing values, wrong data types, inconsistent labels. A thorough inspection before any analysis saves significant time later.\n# Check data types and non-null counts\nprint(df.info())\n\n# Count missing values in each column\nprint(df.isnull().sum())\n\n# Get a feel for the range and distribution of numeric columns\nprint(df.describe())\nThe .info() call is your first diagnostic. It shows you the column names, their data types, and how many non-null values exist. If a numeric column appears as object (Python string), that’s a signal that something—a stray text value, a formatting character—is blocking type inference and needs to be fixed before you compute statistics.\nFor the Iris dataset you’ll find no missing values and clean types, but in real-world data you might drop rows with df.dropna(), fill gaps with df.fillna(), or cast a column with df['column'] = df['column'].astype(float). Documenting every cleaning decision—even in a comment—keeps your analysis reproducible."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-3-summarize-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-3-summarize-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 3: Summarize the Data",
    "text": "Step 3: Summarize the Data\nSummary statistics give you a numerical map of your dataset. The .describe() method returns the mean, standard deviation, min, max, and quartiles for every numeric column at once. For categorical columns, you’ll want value counts.\n# Numeric summary\nprint(df.describe().round(2))\n\n# Count observations per species\nprint(df[\"species\"].value_counts())\n\n# Grouped means: how do measurements differ by species?\nprint(df.groupby(\"species\").mean().round(2))\nThe grouped mean is where EDA gets interesting. Even this simple call reveals that setosa flowers have noticeably shorter and narrower petals than the other two species—a pattern that will jump out even more clearly once you plot it. This is the moment where data starts becoming insight.\nThe table below summarizes the main pandas methods we’ve used and what each one tells you:\n\n\n\nMethod\nWhat It Returns\n\n\n\n\ndf.shape\n(rows, columns) — basic size check\n\n\ndf.info()\nColumn names, dtypes, non-null counts\n\n\ndf.isnull().sum()\nMissing value count per column\n\n\ndf.describe()\nCount, mean, std, min, quartiles, max\n\n\ndf.value_counts()\nFrequency of each unique value\n\n\ndf.groupby().mean()\nColumn means broken out by a category"
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-4-visualize-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-4-visualize-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 4: Visualize the Data",
    "text": "Step 4: Visualize the Data\nNumbers summarize; plots reveal. Even a simple scatter plot can expose relationships that a table of means hides. We’ll use seaborn’s high-level functions to produce clean, labeled figures with minimal code.\n# Scatter plot: sepal length vs. petal length, colored by species\nsns.scatterplot(\n    data=df,\n    x=\"sepal_length\",\n    y=\"petal_length\",\n    hue=\"species\",\n    palette=\"Set2\"\n)\nplt.title(\"Sepal Length vs. Petal Length by Species\")\nplt.xlabel(\"Sepal Length (cm)\")\nplt.ylabel(\"Petal Length (cm)\")\nplt.tight_layout()\nplt.savefig(\"scatter_iris.png\", dpi=150)\nplt.show()\nA few habits to build from the start: always label your axes, always include a title, and always call plt.tight_layout() before saving so axis labels don’t get clipped. These take five extra seconds and make your figures look professional every time.\nFor a quick overview of all numeric columns at once, a pair plot is invaluable:\n# Pair plot: all pairwise relationships, colored by species\nsns.pairplot(df, hue=\"species\", palette=\"Set2\")\nplt.savefig(\"pairplot_iris.png\", dpi=150)\nplt.show()\nThe pair plot immediately shows that petal length and petal width are the strongest separators between species—information that would take several separate analyses to piece together from tables alone."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#conclusion-and-next-steps",
    "href": "posts/python-data-workflow-tutorial.html#conclusion-and-next-steps",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nYou now have a complete, repeatable workflow for Python data analysis: load → inspect → clean → summarize → visualize. Each step builds on the last, and the habits you establish here—checking data shape on load, documenting cleaning decisions, labeling every plot—compound into cleaner, more trustworthy analyses over time.\nThis tutorial covers the exploratory phase, but it’s only the beginning. Natural next steps include:\n\nBuilding statistical models with scikit-learn once you understand your features\nAutomating reports by embedding this workflow in a Quarto notebook\nPulling data from APIs or databases instead of local files using requests or sqlalchemy\n\nIf you try this workflow on your own dataset, I’d love to hear what you find—what patterns showed up, what cleaning challenges you hit, or where this template fell short. Share your experience or questions in the comments, or reach out directly. The best way to get better at data analysis is to keep doing it.\n\nThis tutorial is part of my STAT 386 Data Science Process portfolio at BYU."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Headshot of Golden Huang\n\n\n\n\nHi! I’m Golden Huang, an undergraduate student at Brigham Young University studying Statistics with an emphasis in Data Science. I’m interested in applying data science, machine learning, and statistical modeling to real-world problems, especially in education, technology, and decision-making systems. I enjoy building things that turn data into actionable insight.\n\n\n\n\nBrigham Young University (BYU)\nB.S. in Statistics\nExpected graduation: 2028\n\n\n\n\n\nUndergraduate Research Assistant — BYU\nConduct research at the intersection of AI and education, focusing on personalized learning systems.\nAI in Education Project (EduQuest)\nWorking on a research-to-tech project that explores goal-driven, project-based learning supported by AI-generated personalized assignments.\nStudent Leadership & Event Organization\nOrganized and led large-scale cultural and academic events through student organizations, coordinating logistics, marketing, and cross-team collaboration.\n\n\n\n\n\nProgramming: Python, R, JavaScript\nData Science: Pandas, NumPy, data visualization, statistical modeling\n\nTools: Git, GitHub, Jupyter Notebooks, Markdown\n\n\n\n\nOutside of academics, I enjoy strength training, kickboxing, and exploring productivity systems. I’m also interested in entrepreneurship, education reform, and building tools that help people learn more effectively.\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Golden Huang, an undergraduate student at Brigham Young University studying Statistics with an emphasis in Data Science. I’m interested in applying data science, machine learning, and statistical modeling to real-world problems, especially in education, technology, and decision-making systems. I enjoy building things that turn data into actionable insight."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Brigham Young University (BYU)\nB.S. in Statistics\nExpected graduation: 2028"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Undergraduate Research Assistant — BYU\nConduct research at the intersection of AI and education, focusing on personalized learning systems.\nAI in Education Project (EduQuest)\nWorking on a research-to-tech project that explores goal-driven, project-based learning supported by AI-generated personalized assignments.\nStudent Leadership & Event Organization\nOrganized and led large-scale cultural and academic events through student organizations, coordinating logistics, marketing, and cross-team collaboration."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "",
    "text": "Programming: Python, R, JavaScript\nData Science: Pandas, NumPy, data visualization, statistical modeling\n\nTools: Git, GitHub, Jupyter Notebooks, Markdown"
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I enjoy strength training, kickboxing, and exploring productivity systems. I’m also interested in entrepreneurship, education reform, and building tools that help people learn more effectively.\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  }
]