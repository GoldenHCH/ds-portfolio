[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "From Raw Data to Insight: A Python Data Analysis Workflow\n\n\n\npython\n\ndata analysis\n\ntutorial\n\n\n\n\n\n\n\n\n\nFeb 20, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "",
    "text": "I’m a Statistics student at BYU taking STAT 386: Data Science Process.\nThis site is my course portfolio, where I share: - Data analysis projects - Tutorial-style blog posts - My final project for the course\nUse the navigation bar to explore my work."
  },
  {
    "objectID": "index.html#hi-im-golden",
    "href": "index.html#hi-im-golden",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "",
    "text": "I’m a Statistics student at BYU taking STAT 386: Data Science Process.\nThis site is my course portfolio, where I share: - Data analysis projects - Tutorial-style blog posts - My final project for the course\nUse the navigation bar to explore my work."
  },
  {
    "objectID": "index.html#latest-tutorial",
    "href": "index.html#latest-tutorial",
    "title": "Golden Huang – STAT 386 Portfolio",
    "section": "Latest Tutorial",
    "text": "Latest Tutorial\nFrom Raw Data to Insight: A Python Data Analysis Workflow\nA repeatable Python workflow for exploratory data analysis — loading data, cleaning, summarizing, and visualizing with pandas, matplotlib, and seaborn."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html",
    "href": "posts/python-data-workflow-tutorial.html",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "",
    "text": "Every data science project starts the same way: you have a messy dataset and a question you want to answer. The gap between those two things is the data analysis workflow—the sequence of steps that takes you from raw numbers in a spreadsheet to a clear, communicable insight.\nIn this tutorial, I’ll walk you through a repeatable Python workflow for exploratory data analysis (EDA). We’ll cover loading data, inspecting and cleaning it, computing summary statistics, and creating visualizations. By the end, you’ll have a practical template you can apply to almost any dataset you encounter, whether it comes from a CSV file, an API, or a database export.\nThe tools we’ll use are pandas for data manipulation and matplotlib plus seaborn for visualization. If you’ve written any Python before, you’ll find these libraries approachable, and the patterns we establish here transfer directly into more advanced machine learning pipelines later on."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#introduction",
    "href": "posts/python-data-workflow-tutorial.html#introduction",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "",
    "text": "Every data science project starts the same way: you have a messy dataset and a question you want to answer. The gap between those two things is the data analysis workflow—the sequence of steps that takes you from raw numbers in a spreadsheet to a clear, communicable insight.\nIn this tutorial, I’ll walk you through a repeatable Python workflow for exploratory data analysis (EDA). We’ll cover loading data, inspecting and cleaning it, computing summary statistics, and creating visualizations. By the end, you’ll have a practical template you can apply to almost any dataset you encounter, whether it comes from a CSV file, an API, or a database export.\nThe tools we’ll use are pandas for data manipulation and matplotlib plus seaborn for visualization. If you’ve written any Python before, you’ll find these libraries approachable, and the patterns we establish here transfer directly into more advanced machine learning pipelines later on."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#prerequisites",
    "href": "posts/python-data-workflow-tutorial.html#prerequisites",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstall packages\nBefore running any code, make sure Python 3.9 or newer is installed. Run this in your terminal to install the required libraries:\npython -m pip install pandas matplotlib seaborn\n\n\nGet a dataset\nFor this tutorial, we’ll use the well-known Iris dataset, which ships with seaborn’s built-in load_dataset and requires no external download. The same steps apply to any tabular data you load from a local CSV using pandas read_csv."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-1-load-your-data",
    "href": "posts/python-data-workflow-tutorial.html#step-1-load-your-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 1: Load Your Data",
    "text": "Step 1: Load Your Data\nThe first step is always getting data into Python. For a CSV file you’ve downloaded locally, use pandas.read_csv. For data bundled with a library like seaborn, use its built-in loader.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Iris dataset (built into seaborn — no file needed)\ndf = sns.load_dataset(\"iris\")\n\n# Confirm it loaded correctly\nprint(df.shape)   # (150, 5): 150 rows, 5 columns\nprint(df.head())  # Preview first five rows\nLoading the data is simple, but don’t skip the .shape and .head() calls. These two lines tell you immediately whether the data arrived intact—correct number of rows, recognizable column names, and plausible values. Skipping them is how you end up debugging mysterious errors twenty steps later."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-2-inspect-and-clean-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-2-inspect-and-clean-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 2: Inspect and Clean the Data",
    "text": "Step 2: Inspect and Clean the Data\nRaw data almost always has issues: missing values, wrong data types, inconsistent labels. A thorough inspection before any analysis saves significant time later.\n\nQuick diagnostics: .info() and missingness\nThe pandas DataFrame.info method is your first diagnostic. It shows column names, data types, and non-null counts in one call. If a numeric column appears as object (a Python string type), that signals a stray text value or formatting character that blocks type inference and must be fixed before you compute statistics.\n# Check data types and non-null counts\nprint(df.info())\n\n# Count missing values in each column\nprint(df.isnull().sum())\n\n# Get a feel for the range and distribution of numeric columns\nprint(df.describe())\n\n\nCommon cleaning moves (real-world note)\nFor the Iris dataset you’ll find no missing values and clean types, but in real-world data you’ll encounter both. The three most common fixes are:\n\nDrop rows with df.dropna() when missing data is sparse and random.\nFill gaps with df.fillna(value) when you have a principled imputation value (e.g., median or zero).\nCast a column with df['column'] = df['column'].astype(float) when a numeric column was read as a string.\n\nDocument every cleaning decision—even in a comment—so your analysis stays reproducible."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-3-summarize-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-3-summarize-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 3: Summarize the Data",
    "text": "Step 3: Summarize the Data\nSummary statistics give you a numerical map of your dataset. The .describe() method returns the mean, standard deviation, min, max, and quartiles for every numeric column at once. For categorical columns, value counts are the equivalent snapshot.\n# Numeric summary\nprint(df.describe().round(2))\n\n# Count observations per species\nprint(df[\"species\"].value_counts())\n\n# Grouped means: how do measurements differ by species?\nprint(df.groupby(\"species\").mean().round(2))\nThe grouped mean is where EDA gets interesting. Even this simple call reveals that setosa flowers have noticeably shorter and narrower petals than the other two species—a pattern that will jump out even more clearly once you plot it. This is the moment where data starts becoming insight.\nThe table below summarizes the main pandas methods used in this workflow and what each one tells you:\n\n\n\nMethod\nWhat It Returns\n\n\n\n\ndf.shape\n(rows, columns) — basic size check\n\n\ndf.info()\nColumn names, dtypes, non-null counts\n\n\ndf.isnull().sum()\nMissing value count per column\n\n\ndf.describe()\nCount, mean, std, min, quartiles, max\n\n\ndf.value_counts()\nFrequency of each unique value\n\n\ndf.groupby().mean()\nColumn means broken out by a category"
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#step-4-visualize-the-data",
    "href": "posts/python-data-workflow-tutorial.html#step-4-visualize-the-data",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Step 4: Visualize the Data",
    "text": "Step 4: Visualize the Data\nNumbers summarize; plots reveal. Even a simple scatter plot can expose relationships that a table of means hides. We’ll use seaborn’s scatterplot and seaborn’s pairplot to produce clean, labeled figures with minimal code.\n\nScatter plot (what to look for)\n# Scatter plot: sepal length vs. petal length, colored by species\nsns.scatterplot(\n    data=df,\n    x=\"sepal_length\",\n    y=\"petal_length\",\n    hue=\"species\",\n    palette=\"Set2\"\n)\nplt.title(\"Sepal Length vs. Petal Length by Species\")\nplt.xlabel(\"Sepal Length (cm)\")\nplt.ylabel(\"Petal Length (cm)\")\nplt.tight_layout()\nplt.savefig(\"images/scatter_iris.png\", dpi=150)\nplt.show()\n\n\n\nScatter plot of sepal length vs petal length, colored by Iris species (setosa, versicolor, virginica)\n\n\nA few habits to build from the start: always label your axes, always include a title, and always call plt.tight_layout() before saving so axis labels don’t get clipped. These take five extra seconds and make your figures look professional every time.\n\n\nPair plot (interpretation + caveats)\nFor a quick overview of all numeric columns at once, a pair plot is invaluable:\n# Pair plot: all pairwise relationships, colored by species\nsns.pairplot(df, hue=\"species\", palette=\"Set2\")\nplt.savefig(\"images/pairplot_iris.png\", dpi=150)\nplt.show()\nThe pair plot immediately shows that petal length and petal width are the strongest separators between species—information that would take several separate analyses to piece together from tables alone. One caveat: pair plots grow quadratically with the number of columns, so for wide datasets (20+ features) consider a correlation heatmap instead."
  },
  {
    "objectID": "posts/python-data-workflow-tutorial.html#conclusion-and-next-steps",
    "href": "posts/python-data-workflow-tutorial.html#conclusion-and-next-steps",
    "title": "From Raw Data to Insight: A Python Data Analysis Workflow",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nYou now have a complete, repeatable workflow for Python data analysis: load → inspect → clean → summarize → visualize. Each step builds on the last, and the habits you establish here—checking data shape on load, documenting cleaning decisions, labeling every plot—compound into cleaner, more trustworthy analyses over time.\nThis tutorial covers the exploratory phase, but it’s only the beginning. Natural next steps include:\n\nBuilding statistical models with scikit-learn once you understand your features\nAutomating reports by embedding this workflow in a Quarto notebook\nPulling data from APIs or databases instead of local files using requests or sqlalchemy\n\nTry this workflow on your own dataset and share what you find. Post a screenshot of your most interesting plot or push your adapted notebook to GitHub—linking your result is a great way to build a data science portfolio one project at a time.\n\nThis tutorial is part of my STAT 386 Data Science Process portfolio at BYU."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Headshot of Golden Huang\n\n\n\n\nHi! I’m Golden Huang, an undergraduate student at Brigham Young University studying Statistics with an emphasis in Data Science. I’m interested in applying data science, machine learning, and statistical modeling to real-world problems, especially in education, technology, and decision-making systems. I enjoy building things that turn data into actionable insight.\n\n\n\n\nBrigham Young University (BYU)\nB.S. in Statistics\nExpected graduation: 2028\n\n\n\n\n\nUndergraduate Research Assistant — BYU\nConduct research at the intersection of AI and education, focusing on personalized learning systems.\nAI in Education Project (EduQuest)\nWorking on a research-to-tech project that explores goal-driven, project-based learning supported by AI-generated personalized assignments.\nStudent Leadership & Event Organization\nOrganized and led large-scale cultural and academic events through student organizations, coordinating logistics, marketing, and cross-team collaboration.\n\n\n\n\n\nProgramming: Python, R, JavaScript\nData Science: Pandas, NumPy, data visualization, statistical modeling\n\nTools: Git, GitHub, Jupyter Notebooks, Markdown\n\n\n\n\nOutside of academics, I enjoy strength training, kickboxing, and exploring productivity systems. I’m also interested in entrepreneurship, education reform, and building tools that help people learn more effectively.\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Golden Huang, an undergraduate student at Brigham Young University studying Statistics with an emphasis in Data Science. I’m interested in applying data science, machine learning, and statistical modeling to real-world problems, especially in education, technology, and decision-making systems. I enjoy building things that turn data into actionable insight."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Brigham Young University (BYU)\nB.S. in Statistics\nExpected graduation: 2028"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Undergraduate Research Assistant — BYU\nConduct research at the intersection of AI and education, focusing on personalized learning systems.\nAI in Education Project (EduQuest)\nWorking on a research-to-tech project that explores goal-driven, project-based learning supported by AI-generated personalized assignments.\nStudent Leadership & Event Organization\nOrganized and led large-scale cultural and academic events through student organizations, coordinating logistics, marketing, and cross-team collaboration."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "",
    "text": "Programming: Python, R, JavaScript\nData Science: Pandas, NumPy, data visualization, statistical modeling\n\nTools: Git, GitHub, Jupyter Notebooks, Markdown"
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I enjoy strength training, kickboxing, and exploring productivity systems. I’m also interested in entrepreneurship, education reform, and building tools that help people learn more effectively.\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  }
]