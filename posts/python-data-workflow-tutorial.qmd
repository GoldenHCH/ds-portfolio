---
title: "From Raw Data to Insight: A Python Data Analysis Workflow"
date: 2026-02-20
categories: [python, data analysis, tutorial]
---

## Introduction

Every data science project starts the same way: you have a messy dataset and a question you want to answer. The gap between those two things is the data analysis workflow—the sequence of steps that takes you from raw numbers in a spreadsheet to a clear, communicable insight.

In this tutorial, I'll walk you through a repeatable Python workflow for exploratory data analysis (EDA). We'll cover loading data, inspecting and cleaning it, computing summary statistics, and creating visualizations. By the end, you'll have a practical template you can apply to almost any dataset you encounter, whether it comes from a CSV file, an API, or a database export.

The tools we'll use are [pandas](https://pandas.pydata.org/docs/getting_started/index.html) for data manipulation and [matplotlib](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) plus [seaborn](https://seaborn.pydata.org/) for visualization. If you've written any Python before, you'll find these libraries approachable, and the patterns we establish here transfer directly into more advanced machine learning pipelines later on.

## Prerequisites

### Install packages

Before running any code, make sure Python 3.9 or newer is installed. Run this in your terminal to install the required libraries:

```bash
python -m pip install pandas matplotlib seaborn
```

### Get a dataset

For this tutorial, we'll use the well-known Iris dataset, which ships with [seaborn's built-in `load_dataset`](https://seaborn.pydata.org/generated/seaborn.load_dataset.html) and requires no external download. The same steps apply to any tabular data you load from a local CSV using [pandas `read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

## Step 1: Load Your Data

The first step is always getting data into Python. For a CSV file you've downloaded locally, use `pandas.read_csv`. For data bundled with a library like seaborn, use its built-in loader.

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Iris dataset (built into seaborn — no file needed)
df = sns.load_dataset("iris")

# Confirm it loaded correctly
print(df.shape)   # (150, 5): 150 rows, 5 columns
print(df.head())  # Preview first five rows
```

Loading the data is simple, but don't skip the `.shape` and `.head()` calls. These two lines tell you immediately whether the data arrived intact—correct number of rows, recognizable column names, and plausible values. Skipping them is how you end up debugging mysterious errors twenty steps later.

## Step 2: Inspect and Clean the Data

Raw data almost always has issues: missing values, wrong data types, inconsistent labels. A thorough inspection before any analysis saves significant time later.

### Quick diagnostics: .info() and missingness

The [pandas `DataFrame.info`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method is your first diagnostic. It shows column names, data types, and non-null counts in one call. If a numeric column appears as `object` (a Python string type), that signals a stray text value or formatting character that blocks type inference and must be fixed before you compute statistics.

```python
# Check data types and non-null counts
print(df.info())

# Count missing values in each column
print(df.isnull().sum())

# Get a feel for the range and distribution of numeric columns
print(df.describe())
```

### Common cleaning moves (real-world note)

For the Iris dataset you'll find no missing values and clean types, but in real-world data you'll encounter both. The three most common fixes are:

- Drop rows with `df.dropna()` when missing data is sparse and random.
- Fill gaps with `df.fillna(value)` when you have a principled imputation value (e.g., median or zero).
- Cast a column with `df['column'] = df['column'].astype(float)` when a numeric column was read as a string.

Document every cleaning decision—even in a comment—so your analysis stays reproducible.

## Step 3: Summarize the Data

Summary statistics give you a numerical map of your dataset. The `.describe()` method returns the mean, standard deviation, min, max, and quartiles for every numeric column at once. For categorical columns, value counts are the equivalent snapshot.

```python
# Numeric summary
print(df.describe().round(2))

# Count observations per species
print(df["species"].value_counts())

# Grouped means: how do measurements differ by species?
print(df.groupby("species").mean().round(2))
```

The grouped mean is where EDA gets interesting. Even this simple call reveals that setosa flowers have noticeably shorter and narrower petals than the other two species—a pattern that will jump out even more clearly once you plot it. This is the moment where data starts becoming insight.

The table below summarizes the main pandas methods used in this workflow and what each one tells you:

| Method | What It Returns |
|---|---|
| `df.shape` | (rows, columns) — basic size check |
| `df.info()` | Column names, dtypes, non-null counts |
| `df.isnull().sum()` | Missing value count per column |
| `df.describe()` | Count, mean, std, min, quartiles, max |
| `df.value_counts()` | Frequency of each unique value |
| `df.groupby().mean()` | Column means broken out by a category |

## Step 4: Visualize the Data

Numbers summarize; plots reveal. Even a simple scatter plot can expose relationships that a table of means hides. We'll use [seaborn's `scatterplot`](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) and [seaborn's `pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to produce clean, labeled figures with minimal code.

### Scatter plot (what to look for)

```python
# Scatter plot: sepal length vs. petal length, colored by species
sns.scatterplot(
    data=df,
    x="sepal_length",
    y="petal_length",
    hue="species",
    palette="Set2"
)
plt.title("Sepal Length vs. Petal Length by Species")
plt.xlabel("Sepal Length (cm)")
plt.ylabel("Petal Length (cm)")
plt.tight_layout()
plt.savefig("images/scatter_iris.png", dpi=150)
plt.show()
```

![Scatter plot of sepal length vs petal length, colored by Iris species (setosa, versicolor, virginica)](images/scatter_iris.png){fig-alt="Scatter plot showing sepal length on the x-axis and petal length on the y-axis. Points are colored by species: setosa clusters at lower values, while versicolor and virginica overlap at higher values with a clear linear trend."}

A few habits to build from the start: always label your axes, always include a title, and always call `plt.tight_layout()` before saving so axis labels don't get clipped. These take five extra seconds and make your figures look professional every time.

### Pair plot (interpretation + caveats)

For a quick overview of all numeric columns at once, a pair plot is invaluable:

```python
# Pair plot: all pairwise relationships, colored by species
sns.pairplot(df, hue="species", palette="Set2")
plt.savefig("images/pairplot_iris.png", dpi=150)
plt.show()
```

The pair plot immediately shows that petal length and petal width are the strongest separators between species—information that would take several separate analyses to piece together from tables alone. One caveat: pair plots grow quadratically with the number of columns, so for wide datasets (20+ features) consider a correlation heatmap instead.

## Conclusion and Next Steps

You now have a complete, repeatable workflow for Python data analysis: load → inspect → clean → summarize → visualize. Each step builds on the last, and the habits you establish here—checking data shape on load, documenting cleaning decisions, labeling every plot—compound into cleaner, more trustworthy analyses over time.

This tutorial covers the exploratory phase, but it's only the beginning. Natural next steps include:

- Building statistical models with `scikit-learn` once you understand your features
- Automating reports by embedding this workflow in a Quarto notebook
- Pulling data from APIs or databases instead of local files using `requests` or `sqlalchemy`

Try this workflow on your own dataset and share what you find. Post a screenshot of your most interesting plot or push your adapted notebook to GitHub—linking your result is a great way to build a data science portfolio one project at a time.

---

*This tutorial is part of my STAT 386 Data Science Process portfolio at BYU.*
